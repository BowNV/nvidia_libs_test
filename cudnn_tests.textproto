# This file contains custom test configs, in addition to the ones generated in
# the source code.

# This TRUE_HALF_CONFIG in NHWC format is not supported according to the
# documentation, but the failure mode when trying to run IMPLICIT_PRECOMP_GEMM
# (after successfully calling GetWorkspaceSize) is unexpected:
#
# cuDNN 6.0.21 fails to run the convolution with STATUS_INTERNAL_ERROR.
#
# See nvbugs/2071663. Resolution: will not fix.
convolution_test {
  reference {
    input {
      dimension: 1
      dimension: 1
      dimension: 128
      dimension: 128
      format: TENSOR_NHWC
      data_type: DATA_HALF
    }
    filter {
      dimension: 1
      dimension: 1
      dimension: 3
      dimension: 3
      format: TENSOR_NHWC
      data_type: DATA_HALF
    }
    convolution {
      pad: 1
      pad: 1
      compute_mode: DATA_FLOAT
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "NHWC_TRUE_HALF_CONFIG_Unsupported"
  }
  test {
    convolution {
      compute_mode: DATA_HALF
    }
  }
}

# This FLOAT_CONFIG in NHWC format fails with STATUS_INTERNAL_ERROR on cuDNN
# 6.0.21. Fixed in cuDNN 7.
#
# See nvbugs/2071665. Resolution: will not fix.
convolution_test {
  reference {
    input {
      dimension: 1
      dimension: 16
      dimension: 81
      dimension: 47
      data_type: DATA_FLOAT
      format: TENSOR_NHWC
    }
    filter {
      dimension: 1
      dimension: 16
      dimension: 8
      dimension: 11
      data_type: DATA_FLOAT
      format: TENSOR_NHWC
    }
    convolution {
      pad: 4
      pad: 5
      compute_mode: DATA_FLOAT
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "NHWC_FLOAT_CONFIG_InternalError"
  }
}

# This PSEUDO_HALF_CONFIG in NHWC format crashes in GetWorkspaceSize on cuDNN
# 6.0.21. Fixed in cuDNN 7.
#
# See nvbugs/2071668.
convolution_test {
  reference {
    input {
      dimension: 1
      dimension: 1
      dimension: 128
      dimension: 128
      format: TENSOR_NHWC
      data_type: DATA_HALF
    }
    filter {
      dimension: 1
      dimension: 1
      dimension: 3
      dimension: 3
      format: TENSOR_NHWC
      data_type: DATA_HALF
    }
    convolution {
      pad: 1
      pad: 1
      compute_mode: DATA_FLOAT
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "NHWC_PSEUDO_HALF_CONFIG_Crash"
  }
}

# This DOUBLE_CONFIG in NCHW format seems to produce incorrect results for
# CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING. Tested with cuDNN 7.0.5.
#
# See nvbugs/2072856.
convolution_test {
  reference {
    input {
      dimension: 1025
      dimension: 64
      dimension: 7
      dimension: 33
      format: TENSOR_NCHW
      data_type: DATA_DOUBLE
    }
    filter {
      dimension: 96
      dimension: 64
      dimension: 7
      dimension: 11
      format: TENSOR_NCHW
      data_type: DATA_DOUBLE
    }
    convolution {
      pad: 0
      pad: 0
      compute_mode: DATA_DOUBLE
    }
    bwd_filter_algo: CONVOLUTION_BWD_FILTER_ALGO_1
    label: "CONVOLUTION_BWD_FILTER_1025x64x7x33_96x64x7x11_VALID_Incorrect"
  }
  test {
    bwd_filter_algo: CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING
  }
}

# This DOUBLE_CONFIG in NCHW format fails with STATUS_EXECUTION_FAILED on
# cuDNN 7.1.1, tested with V100-SXM2.
#
# See nvbugs/2072858. cuDNN 7.1.2 returns STATUS_NOT_SUPPORTED.
#
# Ideally, cudnnGetWorkspaceSize would already return STATUS_NOT_SUPPORTED,
# but NVIDIA says that will take a while to fix. They will mention this
# limitation in the documentation of the next release.
#
# See nvbugs/2082072.
convolution_test {
  reference {
    input {
      dimension: 29
      dimension: 2
      dimension: 864
      dimension: 1556
      format: TENSOR_NCHW
      data_type: DATA_DOUBLE
    }
    filter {
      dimension: 2
      dimension: 2
      dimension: 3
      dimension: 3
      format: TENSOR_NCHW
      data_type: DATA_DOUBLE
    }
    convolution {
      pad: 1
      pad: 1
      compute_mode: DATA_DOUBLE
    }
    bwd_filter_algo: CONVOLUTION_BWD_FILTER_ALGO_1
    label: "CONVOLUTION_BWD_FILTER_NCHW_TRUE_HALF_29x2x864x1556_2x2x3x3_SAME_Fails"
  }
}

# This TENSOR_OP_CONFIG in NCHW format returns some algorithms twice from
# cudnnGetConvolutionForwardAlgorithm_v7, once for DEFAULT_MATH and once for
# TENSOR_OP_MATH. I wasn't aware that a mathType member was added to
# cudnnConvolution*AlgoPerf_t in cuDNN 7 to distinguish the two cases. So
# cuDNN works as intended, and the ConvolutionTest.GetAlgorithm_v7 test now
# handles it correctly.
#
# See nvbugs/2072859, works as intended.
convolution_test {
  reference {
    input {
      dimension: 52
      dimension: 7
      dimension: 112
      dimension: 4
      data_type: DATA_DOUBLE
      format: TENSOR_NCHW
    }
    filter {
      dimension: 873
      dimension: 7
      dimension: 3
      dimension: 3
      data_type: DATA_DOUBLE
      format: TENSOR_NCHW
    }
    convolution {
      pad: 0
      pad: 0
      compute_mode: DATA_DOUBLE
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "CONVOLUTION_FWD_NCHW_TENSOR_OP_52x7x112x4_873x7x3x3_VALID_GetAlgo_v7"
  }
  test {
    input {
      data_type: DATA_HALF
      format: TENSOR_NCHW
    }
    filter {
      data_type: DATA_HALF
      format: TENSOR_NCHW
    }
    convolution {
      compute_mode: DATA_HALF
      math_type: TENSOR_OP_MATH
    }
    all_algos: CONVOLUTION_FWD
    label: "NCHW_TENSOR_OP"
  }
}

# This 3D convolution produces an illegal memory access using the FFT tiling
# algorithm. Tested with cuDNN 7.0.5.
#
# See nvbugs/2138754.
convolution_test {
  reference {
    input {
      dimension: 12
      dimension: 4095
      dimension: 1
      dimension: 1
      dimension: 1

      stride: 4095
      stride: 1
      stride: 1
      stride: 1
      stride: 1

      data_type: DATA_FLOAT
    }
    filter {
      dimension: 4095
      dimension: 4095
      dimension: 1
      dimension: 1
      dimension: 1
      data_type: DATA_FLOAT
      format: TENSOR_NCHW
    }
    output {
      dimension: 12
      dimension: 4095
      dimension: 1
      dimension: 1
      dimension: 1

      stride: 4095
      stride: 1
      stride: 1
      stride: 1
      stride: 1

      data_type: DATA_FLOAT
    }
    convolution {
      pad: 0
      pad: 0
      pad: 0
      compute_mode: DATA_FLOAT
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_FFT_TILING
    # Disabled to prevent crashing on normal runs.
    label: "DISABLED_CONVOLUTION_3D_FWD_12x4095x1x1x1_4095x4095x1x1x1_SAME_Illegal_Address"
  }
}

# This grouped convolution produces a misaligned memory access using the
# backward filter FFT algorithm. Tested with cuDNN 7.1.4.
#
# See nvbugs/2181786.
convolution_test {
  reference {
    input {
      dimension: 7
      dimension: 24
      dimension: 51
      dimension: 23
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    filter {
      dimension: 222
      dimension: 8
      dimension: 1
      dimension: 3
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    convolution {
      pad: 0
      pad: 1
      compute_mode: DATA_FLOAT
      group_count: 3
    }
    bwd_filter_algo: CONVOLUTION_BWD_FILTER_ALGO_FFT
    label: "DISABLED_CONVOLUTION_BWD_FILTER_NCHW_FLOAT_7x24x51x23_222x8x1x3_SAME_Misaligned_Address"
  }
}


# Checks that the testing code handles beta > 0 correctly.
convolution_test {
  reference {
    one_minus_alpha: 0.3
    beta: 0.4
    input {
      dimension: 1
      dimension: 1
      dimension: 128
      dimension: 128
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    filter {
      dimension: 1
      dimension: 1
      dimension: 3
      dimension: 3
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    convolution {
      pad: 1
      pad: 1
      compute_mode: DATA_FLOAT
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "BlendResult"
  }
  test {
    all_algos: CONVOLUTION_FWD
  }
}

# Basic grouped convolution test.
convolution_test {
  reference {
    input {
      dimension: 1
      dimension: 10  # = filter_in_depth * group_count
      dimension: 13
      dimension: 13
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    filter {
      dimension: 35  # = depth_multiplier * group_count
      dimension: 2
      dimension: 3
      dimension: 3
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    output {
      dimension: 1
      dimension: 35  # = filter_out_depth
      dimension: 13
      dimension: 13
      format: TENSOR_NCHW
      data_type: DATA_FLOAT
    }
    convolution {
      pad: 1
      pad: 1
      compute_mode: DATA_FLOAT
      group_count: 5
    }
    fwd_algo: CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM
    label: "GroupedConvolution"
  }
  test {
    all_algos: CONVOLUTION_FWD
  }
}

